# -*- coding: utf-8 -*-
"""Belarus Car Price Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uUB3sPSN9sy5LmlfeZNMr-qbQoYzOx-m

# BELARUS CAR PRICE PREDICTION

The aim of this project is to predict the price of car in Belarus, by analyzing the car features such as brand, year, fuel type, transmission, mileage, drive unit, color, and segment. The dataset has been taken from kaggle. It has 56244 rows and 12 columns.

## IMPORT LIBRARIES
"""

import pandas as pd
import numpy as np
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_percentage_error,mean_squared_error,mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.feature_selection import SelectKBest,chi2

"""## READ DATA"""

df1=pd.read_csv('/content/cars.csv')
df1

"""## DATA PREPROCESSING"""

df1.shape

df1.dtypes

df1.nunique()

#remove model column because there are alot of unique value and when we do encoding
#there is a chance of overfitting of model
df1.drop(['model'],inplace=True,axis=1)

def car(make):
    if make in ['mazda','mg','rover','alfa-romeo','audi','peugeot','chrysler','bmw','aston-martin','jaguar', 'land-rover']:
       return 'Luxury European'
    elif make in ['renault','dacia','citroen','skoda','fiat','seat','volvo','opel', 'seat', 'volkswagen', 'citroen', 'skoda', 'mini', 'smart']:
       return 'Mainstream European'
    elif make in ['gaz','uaz','moskvich','zaz','landa-vaz','izh','raf','bogdan','moskvich', 'uaz', 'luaz', 'wartburg', 'trabant', 'proton', 'fso', 'jac', 'iran-khodro', 'zotye', 'tagaz', 'saipa', 'brilliance']:
       return 'Russian/Eatern European'
    elif make in ['nissan','toyota','honda','sabaru','mazda''mitsubishi','hyundai', 'honda', 'ssangyong', 'suzuki', 'daihatsu', 'kia', 'changan', 'lexus', 'isuzu', 'great-wall', 'daewoo', 'vortex','infiniti', 'byd', 'geely', 'haval', 'acura', 'scion', 'tata', 'datsun', 'ravon', 'proton', 'jac']:
       return 'Asian'
    elif make in ['ford','chevrolrt','dodge','chrysler','jeep','plymouth','lincoln', 'buick', 'saturn', 'pontiac', 'chevrolet']:
       return 'American'
    elif make in ['tesla','mclaren']:
        return 'speciality'
    else:
        return 'other'
df1['maketype']=df1['make'].apply(car)

"""## Exploratory Data Analysis

## Car Make Type
"""

sns.barplot(x=df1['maketype'].unique(),y=df1['maketype'].value_counts())
plt.xticks(rotation=90)
plt.show()

"""## Categorical Variable Distribution"""

fig,ax=plt.subplots(3,2,figsize=(20,15))
sns.countplot(x='condition',data=df1,ax=ax[0,0])
sns.countplot(x='fuel_type',data=df1,ax=ax[0,1])
sns.countplot(x='transmission',data=df1,ax=ax[1,0])
sns.countplot(x='color',data=df1,ax=ax[1,1])
sns.countplot(x='drive_unit',data=df1,ax=ax[2,0])
sns.countplot(x='maketype',data=df1,ax=ax[2,1])
plt.xticks(rotation=90)
plt.show()

"""## Continuous Variable Distribution"""

fig,ax=plt.subplots(1,2,figsize=(20,10))

sns.histplot(x='year',data=df1,ax=ax[0],bins=50)
sns.histplot(x='priceUSD',data=df1,ax=ax[1])

#most of the cars manufactured after 1980

df1=df1[df1['year']>1980]

"""## Price and Make"""

dfp=df1.groupby('make',as_index=False)['priceUSD'].mean()
dfp1=dfp.nlargest(10,'priceUSD')
plt.figure(figsize=(10,6))
sns.barplot(data=dfp1,x='priceUSD',y='make')

"""## Price and Condition"""

sns.lineplot(x='year',y='priceUSD',data=df1,hue='condition')
plt.title('price of cars  by year and condition')
plt.show()

"""## Price and Transmission"""

sns.lineplot(x='year',y='priceUSD',data=df1,hue='transmission')
plt.title('price of cars  by year and transmission')
plt.show()

"""## Price and Fuel Type"""

sns.lineplot(x='year',y='priceUSD',data=df1,hue='fuel_type')
plt.title('price of cars  by year and fuel type')
plt.show()

"""## Price and Drive Unit"""

sns.lineplot(x='year',y='priceUSD',data=df1,hue='drive_unit')
plt.title('price of cars  by year and drive unit')
plt.show()

"""
## Handling missing values"""

df1.isna().sum()

#filling the missing value with mean
df1['volume(cm3)']=df1['volume(cm3)'].fillna(df1['volume(cm3)'].mean())

#drop rows with missing value in drive_unit and segment column
df1.dropna(subset=['drive_unit','segment'],inplace=True)
df1=df1.reset_index(drop=True)
df1

df1.isna().sum()

df1.drop(columns='make',inplace=True)

"""## Label Encoding"""

#label encoding
cols=['condition','fuel_type','transmission','drive_unit','color','maketype','segment']
lb=LabelEncoder()
for i in cols:
 df1[i]=lb.fit_transform(df1[i])
 print(i,df1[i].unique())

df1

"""## Outlier Detection and Removal"""

#outlier detection by ploting box plot
col=['mileage(kilometers)','volume(cm3)','year','priceUSD']
df1[col].plot(kind='box',subplots=True,figsize=(15,7))
plt.show()

#remove outliers
col=['mileage(kilometers)','volume(cm3)','priceUSD']
for i in col:
 q1=df1[i].quantile(0.25)
 q3=df1[i].quantile(0.75)
 iqr=q3-q1
 lowest=q1-1.5*iqr
 highest=q3+1.5*iqr
 df1=df1[~((df1[i]<lowest)|(df1[i]>highest))]

df1.shape

col=['mileage(kilometers)','volume(cm3)','priceUSD']
df1[col].plot(kind='box',subplots=True,figsize=(15,8))
plt.show()

"""## Correlation Heatmap"""

plt.figure(figsize=(15,10))
sns.heatmap(df1.corr(),annot=True,cmap='coolwarm')

"""## Feature Selection using Chi Square Test"""

x=df1.drop(['priceUSD'],axis=1)
y=df1['priceUSD']

#chi square test
chi2_selector=SelectKBest(chi2,k=9)
x_best=chi2_selector.fit_transform(x,y)
x_best.shape

best=chi2_selector.get_support(indices=True)
print("selected features: ",df1.columns[best].tolist())

"""## Train Test Split"""

#split data into training and testing set
x_train,x_test,y_train,y_test=train_test_split(x_best,y,test_size=0.30,random_state=42)

"""# MODEL BUILDING AND EVALUATION

## MULTIPLE LINEAR REGRESSION
"""

#linear regression object
lr1=LinearRegression()
lr1.fit(x_train,y_train)
y_pred1=lr1.predict(x_test)

"""## Model Evaluation"""

print("Error percentage: ",mean_absolute_percentage_error(y_test,y_pred1))
print("R2 score: ",r2_score(y_test,y_pred1))

"""## DECISION TREE REGRESSOR"""

#decision tree regression object
dtr1=DecisionTreeRegressor()

"""## Hyperparameter Tuning Using GridSearchCV"""

#parameters for grid search

param_dt={'max_depth':[2,4,6,8],'max_features':['auto','sqrt','log2'],
          'min_samples_leaf':[1,2,3,4]}

#grid search object
gsv=GridSearchCV(dtr1,param_dt,cv=5,scoring='neg_mean_squared_error')

#fitting the grid search
gsv.fit(x_train,y_train)

#best parameters
print(gsv.best_params_)

dtr1=DecisionTreeRegressor(max_depth=8,max_features='auto',min_samples_leaf=4)
dtr1.fit(x_train,y_train)
y_pred2=dtr1.predict(x_test)

"""## Model Evaluation"""

print("Error percentage: ",mean_absolute_percentage_error(y_test,y_pred2))
print("R2 score: ",r2_score(y_test,y_pred2))

"""## RANDOM FOREST REGRESSOR"""

#random forest object
rf1=RandomForestRegressor()
rf1.fit(x_train,y_train)
y_pred3=rf1.predict(x_test)

"""## Model Evaluation"""

print("Error percentage: ",mean_absolute_percentage_error(y_test,y_pred3))
print("R2 score: ",r2_score(y_test,y_pred3))

mlst=['lr1','dtr1','rf1']
rlst=[r2_score(y_test,y_pred1),r2_score(y_test,y_pred2),r2_score(y_test,y_pred3)]
sns.barplot(x=mlst,y=rlst)
plt.xlabel('Models')
plt.ylabel('R2 Score')
plt.title('Model Accuracy')
plt.show()